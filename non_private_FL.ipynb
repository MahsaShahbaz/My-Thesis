{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64701381-7457-4a7b-bfa1-e16a93c32690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mahsa/Desktop/FL/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mahsa/Desktop/FL/src')\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1af7a88-0099-41d2-80c6-30654f72c7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 10:45:06,815 - INFO - Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-05-29 10:45:06,816 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "from options import args_parser\n",
    "from update_s3_gradient_matching import LocalUpdate\n",
    "from utils import test_inference, average_weights, exp_details\n",
    "from models import CNNMnistRelu, CNNMnistTanh, CNNFashion_MnistRelu, CNNFashion_MnistTanh, CNNCifar10Relu, CNNCifar10Tanh\n",
    "from datasets import get_dataset\n",
    "from torchvision import models\n",
    "from logging_results import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0b3a0c-b70a-451c-b321-a42b7f3afafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model, train_loss, test_log, all_output_gradients):\n",
    "    \"\"\"\n",
    "    Saves the training results, including the model state, losses, test metrics, and output gradients,\n",
    "    using torch.save for better compatibility with PyTorch objects.\n",
    "    \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        train_loss (list): List of training losses.\n",
    "        test_log (list): List containing tuples/logs of test accuracy and loss.\n",
    "        all_output_gradients (list): List of output gradients collected during training.\n",
    "    \"\"\"\n",
    "    save_directory = os.path.expanduser('~/Desktop/FL/src/Inversion_Attack_Results')\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    file_path = os.path.join(save_directory, f'non_private_FL.pth')\n",
    "\n",
    "    results = {\n",
    "        'model_state_dict': model.state_dict(),  # Save model parameters\n",
    "        'train_loss': train_loss,\n",
    "        'test_accuracy': [log[0] for log in test_log],\n",
    "        'test_loss': [log[1] for log in test_log],\n",
    "        'all_output_gradients': all_output_gradients  # Save output gradients\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        torch.save(results, file_path)\n",
    "        print(f\"Results saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save results: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a629d725-4fca-4591-a806-4d7a7fdefb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "# Initialize the parser\n",
    "parser = argparse.ArgumentParser(description='Federated Learning with Differential Privacy')\n",
    "\n",
    "# federated arguments\n",
    "parser.add_argument('--epochs', type=int, default=100, help=\"number of rounds of training\") #50\n",
    "parser.add_argument('--num_users', type=int, default=10, help=\"number of users: K\")\n",
    "parser.add_argument('--frac', type=float, default=0.5, help='the fraction of clients')\n",
    "parser.add_argument('--local_ep', type=int, default=5, help=\"the number of local epochs: E\")\n",
    "parser.add_argument('--local_bs', type=int, default=50, help=\"local batch size: B\")\n",
    "\n",
    "# optimizer arguments\n",
    "parser.add_argument('--optimizer', type=str, default='sgd', help=\"type of optimizer\")\n",
    "parser.add_argument('--lr', type=float, default=0.002, help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum (default: 0.0)')\n",
    "\n",
    "# model arguments\n",
    "parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
    "parser.add_argument('--activation', type=str, default=\"relu\", help='activation')\n",
    "\n",
    "## DP arguments\n",
    "parser.add_argument('--withDP', type=int, default=0, help='WithDP') \n",
    "\n",
    "# dataset arguments\n",
    "parser.add_argument('--dataset', type=str, default='dr', help=\"name of dataset\")\n",
    "parser.add_argument('--num_classes', type=int, default=5, help=\"number of classes\")\n",
    "parser.add_argument('--device', default='cuda:0', help=\"To use cuda, set to a specific GPU ID. Default set to use CPU.\")\n",
    "parser.add_argument('--iid', type=int, default=1, help='Default set to IID. Set to 0 for non-IID.')\n",
    "parser.add_argument('--unequal', type=int, default=0, help='whether to use unequal data splits for non-i.i.d setting (use 0 for equal splits)')\n",
    "parser.add_argument('--sub_dataset_size', type=int, default=-1, help='To reduce original data to a smaller dataset. For experimental purposes.')\n",
    "parser.add_argument('--local_test_split', type=float, default=0.30, help='local_test_split')                    \n",
    "parser.add_argument('--dr_from_np', type=float, default=1, help='for diabetic_retinopathy dataset')                    \n",
    "parser.add_argument('--exp_name', type=str, default=\"exp_results\", help=\"The name of current experiment for logging.\")\n",
    "\n",
    "# Parse the arguments, ignoring unknown ones\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Setting the device to use GPU0 explicitly if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3df928-0c41-421a-bf3d-4476ad90e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train dataset length: 2931\n",
      "Test dataset length: 731\n",
      "First training image shape: torch.Size([3, 224, 224])\n",
      "First training image type: torch.float32\n",
      "First training label type: <class 'torch.Tensor'>\n",
      "First testing image shape: torch.Size([3, 224, 224])\n",
      "First testing image type: torch.float32\n",
      "First testing label type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and user groups\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "print(\"Train dataset length:\", len(train_dataset))\n",
    "print(\"Test dataset length:\", len(test_dataset))\n",
    "\n",
    "\n",
    "# Shape and Type of dataset\n",
    "# Inspect the first sample from the training dataset\n",
    "train_features, train_labels = train_dataset[0]\n",
    "print(\"First training image shape:\", train_features.shape)\n",
    "print(\"First training image type:\", train_features.dtype)\n",
    "print(\"First training label type:\", type(train_labels))\n",
    "\n",
    "# Inspect the first sample from the testing dataset\n",
    "test_features, test_labels = test_dataset[0]\n",
    "print(\"First testing image shape:\", test_features.shape)\n",
    "print(\"First testing image type:\", test_features.dtype)\n",
    "print(\"First testing label type:\", type(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1149d2-9616-42a6-b79b-0525cd4eb219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 111, 111]           1,792\n",
      "              ReLU-2         [-1, 64, 111, 111]               0\n",
      "         MaxPool2d-3           [-1, 64, 55, 55]               0\n",
      "            Conv2d-4           [-1, 16, 55, 55]           1,040\n",
      "              ReLU-5           [-1, 16, 55, 55]               0\n",
      "            Conv2d-6           [-1, 64, 55, 55]           1,088\n",
      "              ReLU-7           [-1, 64, 55, 55]               0\n",
      "            Conv2d-8           [-1, 64, 55, 55]           9,280\n",
      "              ReLU-9           [-1, 64, 55, 55]               0\n",
      "             Fire-10          [-1, 128, 55, 55]               0\n",
      "           Conv2d-11           [-1, 16, 55, 55]           2,064\n",
      "             ReLU-12           [-1, 16, 55, 55]               0\n",
      "           Conv2d-13           [-1, 64, 55, 55]           1,088\n",
      "             ReLU-14           [-1, 64, 55, 55]               0\n",
      "           Conv2d-15           [-1, 64, 55, 55]           9,280\n",
      "             ReLU-16           [-1, 64, 55, 55]               0\n",
      "             Fire-17          [-1, 128, 55, 55]               0\n",
      "        MaxPool2d-18          [-1, 128, 27, 27]               0\n",
      "           Conv2d-19           [-1, 32, 27, 27]           4,128\n",
      "             ReLU-20           [-1, 32, 27, 27]               0\n",
      "           Conv2d-21          [-1, 128, 27, 27]           4,224\n",
      "             ReLU-22          [-1, 128, 27, 27]               0\n",
      "           Conv2d-23          [-1, 128, 27, 27]          36,992\n",
      "             ReLU-24          [-1, 128, 27, 27]               0\n",
      "             Fire-25          [-1, 256, 27, 27]               0\n",
      "           Conv2d-26           [-1, 32, 27, 27]           8,224\n",
      "             ReLU-27           [-1, 32, 27, 27]               0\n",
      "           Conv2d-28          [-1, 128, 27, 27]           4,224\n",
      "             ReLU-29          [-1, 128, 27, 27]               0\n",
      "           Conv2d-30          [-1, 128, 27, 27]          36,992\n",
      "             ReLU-31          [-1, 128, 27, 27]               0\n",
      "             Fire-32          [-1, 256, 27, 27]               0\n",
      "        MaxPool2d-33          [-1, 256, 13, 13]               0\n",
      "           Conv2d-34           [-1, 48, 13, 13]          12,336\n",
      "             ReLU-35           [-1, 48, 13, 13]               0\n",
      "           Conv2d-36          [-1, 192, 13, 13]           9,408\n",
      "             ReLU-37          [-1, 192, 13, 13]               0\n",
      "           Conv2d-38          [-1, 192, 13, 13]          83,136\n",
      "             ReLU-39          [-1, 192, 13, 13]               0\n",
      "             Fire-40          [-1, 384, 13, 13]               0\n",
      "           Conv2d-41           [-1, 48, 13, 13]          18,480\n",
      "             ReLU-42           [-1, 48, 13, 13]               0\n",
      "           Conv2d-43          [-1, 192, 13, 13]           9,408\n",
      "             ReLU-44          [-1, 192, 13, 13]               0\n",
      "           Conv2d-45          [-1, 192, 13, 13]          83,136\n",
      "             ReLU-46          [-1, 192, 13, 13]               0\n",
      "             Fire-47          [-1, 384, 13, 13]               0\n",
      "           Conv2d-48           [-1, 64, 13, 13]          24,640\n",
      "             ReLU-49           [-1, 64, 13, 13]               0\n",
      "           Conv2d-50          [-1, 256, 13, 13]          16,640\n",
      "             ReLU-51          [-1, 256, 13, 13]               0\n",
      "           Conv2d-52          [-1, 256, 13, 13]         147,712\n",
      "             ReLU-53          [-1, 256, 13, 13]               0\n",
      "             Fire-54          [-1, 512, 13, 13]               0\n",
      "           Conv2d-55           [-1, 64, 13, 13]          32,832\n",
      "             ReLU-56           [-1, 64, 13, 13]               0\n",
      "           Conv2d-57          [-1, 256, 13, 13]          16,640\n",
      "             ReLU-58          [-1, 256, 13, 13]               0\n",
      "           Conv2d-59          [-1, 256, 13, 13]         147,712\n",
      "             ReLU-60          [-1, 256, 13, 13]               0\n",
      "             Fire-61          [-1, 512, 13, 13]               0\n",
      "          Dropout-62          [-1, 512, 13, 13]               0\n",
      "           Conv2d-63            [-1, 5, 13, 13]           2,565\n",
      "             ReLU-64            [-1, 5, 13, 13]               0\n",
      "AdaptiveAvgPool2d-65              [-1, 5, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 725,061\n",
      "Trainable params: 725,061\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 51.19\n",
      "Params size (MB): 2.77\n",
      "Estimated Total Size (MB): 54.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# DEVICE SETUP\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BUILD MODEL\n",
    "weights = SqueezeNet1_1_Weights.IMAGENET1K_V1  # Load the predefined weights\n",
    "global_model = squeezenet1_1(weights=weights)  # Initialize the model with weights\n",
    "global_model.classifier[1] = nn.Conv2d(512, 5, kernel_size=(1,1), stride=(1,1))  # Modify the classifier for 5 classes\n",
    "global_model.num_classes = 5\n",
    "global_model.to(device)  # Move the model to the appropriate device\n",
    "summary(global_model, input_size=(3, 224, 224), device=device.type)  # Display the model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6af86e3-5a88-455a-b552-953308008f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Models and Optimizers\n",
    "u_steps = np.zeros(args.num_users)  \n",
    "epsilons = np.zeros(args.num_users)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc8325c-968a-44c2-b791-7fda4189a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Average train loss: 1.0256524360179902\n",
      "Test Accuracy: 66.21%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 2\n",
      "Average train loss: 0.8509734612703322\n",
      "Test Accuracy: 70.45%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 3\n",
      "Average train loss: 0.8482758653163909\n",
      "Test Accuracy: 71.14%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 4\n",
      "Average train loss: 0.7996442997455597\n",
      "Test Accuracy: 70.86%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 5\n",
      "Average train loss: 0.7694860988855361\n",
      "Test Accuracy: 72.37%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 6\n",
      "Average train loss: 0.7411505037546158\n",
      "Test Accuracy: 73.32%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 7\n",
      "Average train loss: 0.7620398336648941\n",
      "Test Accuracy: 74.56%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 8\n",
      "Average train loss: 0.7144153267145157\n",
      "Test Accuracy: 72.64%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 9\n",
      "Average train loss: 0.7317319157719613\n",
      "Test Accuracy: 72.37%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 10\n",
      "Average train loss: 0.746369053721428\n",
      "Test Accuracy: 74.69%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 11\n",
      "Average train loss: 0.6914142918586731\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 12\n",
      "Average train loss: 0.6606687140464782\n",
      "Test Accuracy: 75.92%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 13\n",
      "Average train loss: 0.6053193804621696\n",
      "Test Accuracy: 75.79%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 14\n",
      "Average train loss: 0.6305419689416885\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 15\n",
      "Average train loss: 0.601338016986847\n",
      "Test Accuracy: 75.65%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 16\n",
      "Average train loss: 0.6075714460015298\n",
      "Test Accuracy: 75.79%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 17\n",
      "Average train loss: 0.5971795624494554\n",
      "Test Accuracy: 75.79%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 18\n",
      "Average train loss: 0.5637436893582344\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 19\n",
      "Average train loss: 0.5794548878073693\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 20\n",
      "Average train loss: 0.5728250008821488\n",
      "Test Accuracy: 75.38%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 21\n",
      "Average train loss: 0.5480855545401573\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 22\n",
      "Average train loss: 0.5883036151528358\n",
      "Test Accuracy: 74.28%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 23\n",
      "Average train loss: 0.5589325362443924\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 24\n",
      "Average train loss: 0.5976384165883064\n",
      "Test Accuracy: 75.92%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 25\n",
      "Average train loss: 0.5720047494769096\n",
      "Test Accuracy: 77.70%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 26\n",
      "Average train loss: 0.5164888100326062\n",
      "Test Accuracy: 74.83%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 27\n",
      "Average train loss: 0.5274434238672256\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 28\n",
      "Average train loss: 0.5592272889614105\n",
      "Test Accuracy: 77.15%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 29\n",
      "Average train loss: 0.5070326676964759\n",
      "Test Accuracy: 77.56%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 30\n",
      "Average train loss: 0.49204190492630007\n",
      "Test Accuracy: 76.33%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 31\n",
      "Average train loss: 0.4810480767488479\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 32\n",
      "Average train loss: 0.4872921852767467\n",
      "Test Accuracy: 75.92%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 33\n",
      "Average train loss: 0.4592800894379616\n",
      "Test Accuracy: 76.20%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 34\n",
      "Average train loss: 0.4622562739253044\n",
      "Test Accuracy: 76.33%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 35\n",
      "Average train loss: 0.5156149947643279\n",
      "Test Accuracy: 76.20%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 36\n",
      "Average train loss: 0.49542036399245265\n",
      "Test Accuracy: 76.47%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 37\n",
      "Average train loss: 0.4632589048147201\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 38\n",
      "Average train loss: 0.4290173128247261\n",
      "Test Accuracy: 76.47%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 39\n",
      "Average train loss: 0.44513133957982054\n",
      "Test Accuracy: 76.74%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 40\n",
      "Average train loss: 0.42905721992254253\n",
      "Test Accuracy: 74.97%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 41\n",
      "Average train loss: 0.42064703017473226\n",
      "Test Accuracy: 75.79%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 42\n",
      "Average train loss: 0.38691340938210483\n",
      "Test Accuracy: 77.29%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 43\n",
      "Average train loss: 0.4018906696140766\n",
      "Test Accuracy: 75.38%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 44\n",
      "Average train loss: 0.5025193268060684\n",
      "Test Accuracy: 76.88%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 45\n",
      "Average train loss: 0.44893072530627254\n",
      "Test Accuracy: 76.33%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 46\n",
      "Average train loss: 0.39154784038662915\n",
      "Test Accuracy: 77.70%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 47\n",
      "Average train loss: 0.42942414700984954\n",
      "Test Accuracy: 75.38%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 48\n",
      "Average train loss: 0.4093224802613258\n",
      "Test Accuracy: 76.88%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 49\n",
      "Average train loss: 0.42738412171602247\n",
      "Test Accuracy: 77.84%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 50\n",
      "Average train loss: 0.3725518147647381\n",
      "Test Accuracy: 77.84%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 51\n",
      "Average train loss: 0.3486353415250778\n",
      "Test Accuracy: 77.84%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 52\n",
      "Average train loss: 0.294553442299366\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 53\n",
      "Average train loss: 0.32346042230725286\n",
      "Test Accuracy: 77.02%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 54\n",
      "Average train loss: 0.37842303127050403\n",
      "Test Accuracy: 74.97%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 55\n",
      "Average train loss: 0.34282153367996215\n",
      "Test Accuracy: 77.56%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 56\n",
      "Average train loss: 0.33338670074939725\n",
      "Test Accuracy: 76.47%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 57\n",
      "Average train loss: 0.31289068140089515\n",
      "Test Accuracy: 75.92%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 58\n",
      "Average train loss: 0.34313903480768204\n",
      "Test Accuracy: 76.88%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 59\n",
      "Average train loss: 0.27560501590371134\n",
      "Test Accuracy: 75.92%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 60\n",
      "Average train loss: 0.39579239904880525\n",
      "Test Accuracy: 76.20%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 61\n",
      "Average train loss: 0.2927236489206553\n",
      "Test Accuracy: 77.84%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 62\n",
      "Average train loss: 0.3112064687162638\n",
      "Test Accuracy: 76.06%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 63\n",
      "Average train loss: 0.31207234799861905\n",
      "Test Accuracy: 77.84%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 64\n",
      "Average train loss: 0.283792732283473\n",
      "Test Accuracy: 78.11%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 65\n",
      "Average train loss: 0.2553167659789324\n",
      "Test Accuracy: 77.02%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 66\n",
      "Average train loss: 0.24119006477296354\n",
      "Test Accuracy: 75.38%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 67\n",
      "Average train loss: 0.2814369451999664\n",
      "Test Accuracy: 76.06%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 68\n",
      "Average train loss: 0.2630125433206558\n",
      "Test Accuracy: 77.15%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 69\n",
      "Average train loss: 0.28034072145819666\n",
      "Test Accuracy: 74.97%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 70\n",
      "Average train loss: 0.2746395530179143\n",
      "Test Accuracy: 77.29%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 71\n",
      "Average train loss: 0.26804822772741316\n",
      "Test Accuracy: 74.97%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 72\n",
      "Average train loss: 0.23923676975071428\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 73\n",
      "Average train loss: 0.20427516002207993\n",
      "Test Accuracy: 79.34%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 74\n",
      "Average train loss: 0.20791471911594278\n",
      "Test Accuracy: 76.88%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 75\n",
      "Average train loss: 0.23992813494056459\n",
      "Test Accuracy: 73.05%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 76\n",
      "Average train loss: 0.23983872570097448\n",
      "Test Accuracy: 76.33%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 77\n",
      "Average train loss: 0.2605116662755608\n",
      "Test Accuracy: 73.87%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 78\n",
      "Average train loss: 0.2602562268823385\n",
      "Test Accuracy: 75.79%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 79\n",
      "Average train loss: 0.26847202934324743\n",
      "Test Accuracy: 77.02%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 80\n",
      "Average train loss: 0.20195269919931888\n",
      "Test Accuracy: 77.02%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 81\n",
      "Average train loss: 0.22751176699995992\n",
      "Test Accuracy: 76.61%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 82\n",
      "Average train loss: 0.23138070398941638\n",
      "Test Accuracy: 76.74%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 83\n",
      "Average train loss: 0.20880876937881113\n",
      "Test Accuracy: 77.56%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 84\n",
      "Average train loss: 0.20927327480167152\n",
      "Test Accuracy: 75.65%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 85\n",
      "Average train loss: 0.19512232275679708\n",
      "Test Accuracy: 75.38%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 86\n",
      "Average train loss: 0.2011862673610449\n",
      "Test Accuracy: 74.28%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 87\n",
      "Average train loss: 0.13096422350034118\n",
      "Test Accuracy: 76.20%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 88\n",
      "Average train loss: 0.23118668936192988\n",
      "Test Accuracy: 76.33%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 89\n",
      "Average train loss: 0.1562244239449501\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 90\n",
      "Average train loss: 0.1740690312627703\n",
      "Test Accuracy: 76.88%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 91\n",
      "Average train loss: 0.13482279323972762\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 92\n",
      "Average train loss: 0.13221424876712262\n",
      "Test Accuracy: 75.51%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 93\n",
      "Average train loss: 0.1298256883304566\n",
      "Test Accuracy: 77.15%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 94\n",
      "Average train loss: 0.2435572925209999\n",
      "Test Accuracy: 75.65%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 95\n",
      "Average train loss: 0.17400240343064072\n",
      "Test Accuracy: 76.74%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 96\n",
      "Average train loss: 0.162764749545604\n",
      "Test Accuracy: 77.84%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 97\n",
      "Average train loss: 0.16150941777974367\n",
      "Test Accuracy: 76.20%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 98\n",
      "Average train loss: 0.18707549264654516\n",
      "Test Accuracy: 75.38%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 99\n",
      "Average train loss: 0.09754426405299454\n",
      "Test Accuracy: 76.33%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n",
      "\n",
      "Epoch: 100\n",
      "Average train loss: 0.22402181493118403\n",
      "Test Accuracy: 73.87%\n",
      "Results saved to /home/mahsa/Desktop/FL/src/Inversion_Attack_Results/non_private_FL.pth\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_loss = []\n",
    "test_log = []\n",
    "epsilon_log = []\n",
    "\n",
    "for epoch in range(args.epochs):           \n",
    "    idxs_users = np.random.choice(range(args.num_users),\n",
    "                                  max(int(args.frac * args.num_users), 1),\n",
    "                                  replace=False)\n",
    "    local_weights, local_losses = [], []\n",
    "    all_output_gradients = []\n",
    "     \n",
    "    \n",
    "    for u in idxs_users:\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset, \n",
    "                                  u_id=u, idxs=user_groups[u])\n",
    "        w, loss, u_step, epsilon, user_output_gradients = local_model.update_weights(\n",
    "                                                model=copy.deepcopy(global_model),\n",
    "                                                global_round=epoch,\n",
    "                                                u_step= u_steps[u])\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "        all_output_gradients.extend(user_output_gradients)  # Collect output gradients from each user\n",
    "        u_steps[u] = u_step \n",
    "        epsilons[u] = epsilon\n",
    "\n",
    "    # update global weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # update global weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)        \n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    _acc, _loss = test_inference(args, global_model, test_dataset)        \n",
    "    test_log.append([_acc, _loss])  \n",
    "\n",
    "    if args.withDP:                        \n",
    "        epsilon_log.append(list(epsilons))\n",
    "    else:\n",
    "        epsilon_log = None\n",
    "    \n",
    "    logging(args, epoch, train_loss, test_log, epsilon_log)\n",
    "    \n",
    "    save_results(global_model, train_loss, test_log, all_output_gradients)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
